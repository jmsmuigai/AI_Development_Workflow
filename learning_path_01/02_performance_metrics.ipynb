{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "HODwAKlYqxlT",
      "metadata": {
        "id": "HODwAKlYqxlT"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28c92c51-3635-4e69-a96c-f9107211d983",
      "metadata": {
        "id": "28c92c51-3635-4e69-a96c-f9107211d983"
      },
      "source": [
        "# Inference Metrics: Latency, Throughput, and UX\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/02_performance_metrics.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<p>\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/02_performance_metrics.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/02_performance_metrics.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/02_performance_metrics.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/02_performance_metrics.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/inardini/gcp-nvidia-community-ai-inference-lp/blob/main/learning_path_01/02_performance_metrics.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ne2VgCJ-rJWx",
      "metadata": {
        "id": "Ne2VgCJ-rJWx"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook is a hands-on introduction to the key performance aspects of running AI inference.\n",
        "\n",
        "You’ll learn:\n",
        "\n",
        "- What **latency** and **throughput** mean.\n",
        "- Why these metrics often trade off against each other.\n",
        "- How different parameters (like batch size, prompt length, sampling strategy) affect performance.\n",
        "- How to measure and visualize **p50 vs p90 latency**, **first token (also known as Time to First Token) vs total latency**, and find the \"sweet spot\".\n",
        "- What this means for real-world **user experience**.\n",
        "\n",
        "By the end of this notebook, you'll not only be able to benchmark an AI inference — you'll know what the numbers actually mean and how to tune them for real applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5d5b987-c231-4acb-8793-fb735869ffb0",
      "metadata": {
        "id": "f5d5b987-c231-4acb-8793-fb735869ffb0"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "**Before you begin**, make sure you have:\n",
        "\n",
        "- An NVIDIA GPU environment\n",
        "- Your Hugging Face [access token](https://huggingface.co/settings/token)\n",
        "\n",
        "Let's test which GPUs are available in our system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d1995f8c-23c9-4c65-b470-40b8d3e4d892",
      "metadata": {
        "id": "d1995f8c-23c9-4c65-b470-40b8d3e4d892",
        "outputId": "f24f51df-3852-4e87-f000-1bcc9edea767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov  9 08:09:06 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a59f706b-d849-42ca-beff-5d5d43cf5c52",
      "metadata": {
        "id": "a59f706b-d849-42ca-beff-5d5d43cf5c52"
      },
      "source": [
        "### Authenticating with Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "73ebed9c-8299-43cd-9f1b-d154dcd9cc2a",
      "metadata": {
        "id": "73ebed9c-8299-43cd-9f1b-d154dcd9cc2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4987cfbe06fd42e2951d7075467df45f",
            "bcdf19b88ab74a159615fd5eafc202c3",
            "29c5933c48a849b2aa03c61ef9ebc527",
            "8651add1fcf94edd997c15f9389b2943",
            "3c96dc3c637c40ab82c70a5ee8d48214",
            "1548cef0d62f4aacaa0d7cd00af39934",
            "a462a77199e94aed983aa08ca90fa08a",
            "a9a1d33041294155abfbd223788e680d",
            "c5285a0ac0c046e189699334ea6dbee8",
            "9a80a76bef1c421984241acd563bc05c"
          ]
        },
        "outputId": "1f7988e1-369b-43d1-bb49-70a02bcc02cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(Password(description='HF Token:', layout=Layout(width='450px'), placeholder='paste your Hugging…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4987cfbe06fd42e2951d7075467df45f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5285a0ac0c046e189699334ea6dbee8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ⬇️ Run this cell once\n",
        "from ipywidgets import Password, Button, HBox, Output\n",
        "import os, pathlib\n",
        "import sys\n",
        "\n",
        "from huggingface_hub import HfFolder, whoami\n",
        "\n",
        "# ---- UI widgets ----\n",
        "token_box = Password(\n",
        "    description=\"HF Token:\",\n",
        "    placeholder=\"paste your Hugging Face token here\",\n",
        "    layout={\"width\": \"450px\"},\n",
        ")\n",
        "save_btn = Button(description=\"Save\", button_style=\"success\")\n",
        "out = Output()\n",
        "\n",
        "# ---- Callback ----\n",
        "def save_token(_):\n",
        "    out.clear_output()\n",
        "    token = token_box.value.strip()\n",
        "    with out:\n",
        "        if not token:\n",
        "            print(\"❌ No token entered.\")\n",
        "            return\n",
        "        # Persist token\n",
        "        HfFolder.save_token(token)                 # writes to ~/.cache/huggingface/token\n",
        "        os.environ[\"HF_TOKEN\"] = token             # current kernel env (optional)\n",
        "        # Sanity-check who we are\n",
        "        try:\n",
        "            user = whoami(token)[\"name\"]\n",
        "            print(f\"✅ Token saved. Logged in as: {user}\")\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ Token saved, but user lookup failed:\", e)\n",
        "\n",
        "save_btn.on_click(save_token)\n",
        "\n",
        "display(HBox([token_box, save_btn]), out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0e0b5dd-62a3-4704-9304-0a8f1ca1645a",
      "metadata": {
        "id": "a0e0b5dd-62a3-4704-9304-0a8f1ca1645a"
      },
      "source": [
        "### Loading the model\n",
        "\n",
        "For the experiments in this tutorial we will be using [nvidia/NVIDIA-Nemotron-Nano-9B-v2](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2), a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "253273e9-d8d1-47d3-993c-bcfd546acc5c",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f4bccb87a7c44924937b4345590a6ff3",
            "1f3c5b6f3e2f46a7ac2656843cf831a8",
            "5d24555192ed4cf8b878153ac30af650",
            "07a71c9eee734d51b6ad7e720bfe6da0",
            "3990fd1020ce4ce3ade8050981ba1f0c",
            "fe5a17e537dc4bc6930f740519e53b7d",
            "749bd950745b4df38b0e5dec19f8e5d7",
            "0786f00d19ae41978e7ecaf622a2c8f9",
            "c94dafd4dd2c4b80a16e52ebe7330b61",
            "ad5e9a37d61d42f597fb807921860f47",
            "3157bfa3d948448ba12eebf02778fe10",
            "de5163986aa0454ba75eb9cc5e8dea98",
            "12b6d13a119048d09662a8e23eaecd5f",
            "414d6e2303254546b657f9cad1322d87",
            "a703b9deeebb429c83d9231b37bf271e",
            "4aa94dc1333645b5b4095f97c39e8f20",
            "27d4db0e87e14803a12195289653fead",
            "43210d55e50546d594795ccf47d2ddc8",
            "ccb8168e0ef549e0b0adf64e463e341e",
            "fb6f5076b56640ae97f819f9c5bac626",
            "9226a09cef9d4a3a842c15836dc32a38",
            "3bf3a729f60a4900ac198dbd3e73dcc4",
            "33c05b832ef7403fbe234bca1ab8c788",
            "bc3c9469f7ff4501ad7eddbfa6adac5a",
            "bd3d2f0f3c9a4d55b0ef7be11a37ad0d",
            "ad1bd798abb1468e811583a4170f4dad",
            "cb140bcb9b084230b218d615952d1c0d",
            "8e70e4f7e0c4496ead3e5877d04d895f",
            "a906165a85b3424bbafd4e671d752c4a",
            "61bd8c7bcfae4cc7894dad9c9aac20a7",
            "db09255757ec4808b6699aa9521237e1",
            "d1a903bec7f940fe92f5cad4875072bf",
            "4db67f5605384218a952c3e38375f4a0",
            "daacc8fdb61b4927bf6636e4870c7230",
            "49c27477e1f7474ab77ec87b8c824e53",
            "8cb5a2b092294a9d924f3a3e9ef071d4",
            "e3b7f43ad071486a8c46f04ac0544834",
            "360a1687b25d472f8c81d02a4ee3667e",
            "fbacb77bd01c474abb46f3d95b26b072",
            "9d3885d29c4d4e61b4f59396fe688091",
            "fb7693ff714b48ba850ca7da216ea467",
            "0edf80f715194414b1d506373adfeae9",
            "7917f6c56d5847c2bb1fe94d87f40cbf",
            "6fcc4ec9bad44c8a9274fd1fbce2e5ea",
            "ce887141403e48e0b0aa71172e0f41fa",
            "e68b518536b5464b990eb64ad86463b4",
            "ffd958bf1f7b4a49b86052d3da296b32",
            "36b876a0ec8e449ea6821ca6582ec35e",
            "0a5316227ea345e9a71e47ad8a11899d",
            "384db9377d634ee394a3d177b7fab273",
            "61691f376c8f4b70820436ec08be6af5",
            "0093909427004f278c6588cd9f4ffa23",
            "5a963db7ebae4f4ab7577cb580b1a627",
            "d463ad16e2de4388a81bd588d9f07172",
            "20a263f6919e407682f4a8d98570aa31",
            "47fb0ba1b1e14899b470eedac2d2fc48",
            "4c8d0ab350a44fecbdd15923ec45f127",
            "ced30d1a2da34f43ac4c37817f7846bd",
            "e1c81d824ad8433991bb4a5a3301c41b",
            "b8f136e882f7462b9bbbd3559409108a",
            "004981eb2df84f16bfc0e06c6b409abb",
            "87d4dc8285564d1680b41c883f1b86eb",
            "410733531c4d4102ac01bd535e815471",
            "537ca91504194fc0b2a88ede82bbec4a",
            "6de58b23471b49baa3fc9eaa7e2a5465",
            "59d6e3f02e8442208ff41dcefbf9dc4d"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "253273e9-d8d1-47d3-993c-bcfd546acc5c",
        "outputId": "d53cfab0-c110-4406-89b2-60891ff1ecee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.2/438.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mINFO 11-09 08:10:43 [__init__.py:216] Automatically detected platform cuda.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4bccb87a7c44924937b4345590a6ff3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de5163986aa0454ba75eb9cc5e8dea98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/422 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33c05b832ef7403fbe234bca1ab8c788"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-09 08:10:56 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 131072, 'max_num_seqs': 64, 'disable_log_stats': True, 'mamba_ssm_cache_dtype': 'float32', 'model': 'nvidia/NVIDIA-Nemotron-Nano-9B-v2'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daacc8fdb61b4927bf6636e4870c7230"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "configuration_nemotron_h.py: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce887141403e48e0b0aa71172e0f41fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2:\n",
            "- configuration_nemotron_h.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-09 08:11:21 [model.py:547] Resolved architecture: NemotronHForCausalLM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 11-09 08:11:21 [model.py:1733] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 11-09 08:11:21 [model.py:1510] Using max model len 131072\n",
            "INFO 11-09 08:11:21 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "INFO 11-09 08:11:21 [config.py:297] Hybrid or mamba-based model detected: disabling prefix caching since it is not yet supported.\n",
            "INFO 11-09 08:11:21 [config.py:308] Hybrid or mamba-based model detected: setting cudagraph mode to FULL_AND_PIECEWISE in order to optimize performance.\n",
            "INFO 11-09 08:11:21 [config.py:376] Setting attention block size to 1312 tokens to ensure that attention page size is >= mamba page size.\n",
            "INFO 11-09 08:11:21 [config.py:397] Padding mamba page size by 1.08% to ensure that mamba page size and attention page size are exactly equal.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47fb0ba1b1e14899b470eedac2d2fc48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 11-09 08:11:23 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2385869941.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TP_SIZE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# set >1 if you want tensor parallel across GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m llm = LLM(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float16\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;31m# Create the Engine (autoselects V0 vs V1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    298\u001b[0m             engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Create the LLMEngine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         return cls(vllm_config=vllm_config,\n\u001b[0m\u001b[1;32m    178\u001b[0m                    \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                    \u001b[0mlog_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mengine_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_log_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         self.engine_core = EngineCoreClient.make_client(\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mmultiprocess_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiprocess_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0masyncio_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36mmake_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmultiprocess_mode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masyncio_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSyncMPClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mInprocClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    600\u001b[0m     def __init__(self, vllm_config: VllmConfig, executor_class: type[Executor],\n\u001b[1;32m    601\u001b[0m                  log_stats: bool):\n\u001b[0;32m--> 602\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0masyncio_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0;31m# Engines are managed by this client.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m                 with launch_core_engines(vllm_config, executor_class,\n\u001b[0m\u001b[1;32m    449\u001b[0m                                          \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mengine_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                                                         \u001b[0mcoordinator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py\u001b[0m in \u001b[0;36mlaunch_core_engines\u001b[0;34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;31m# Now wait for engines to start.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         wait_for_engine_startup(\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mhandshake_socket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0maddresses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py\u001b[0m in \u001b[0;36mwait_for_engine_startup\u001b[0;34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcoord_process\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0mfinished\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m             raise RuntimeError(\"Engine core initialization failed. \"\n\u001b[0m\u001b[1;32m    786\u001b[0m                                \u001b[0;34m\"See root cause above. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                                f\"Failed core proc(s): {finished}\")\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if \"vllm\" not in sys.modules:\n",
        "  !pip install --quiet --pre vllm\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from vllm import LLM, SamplingParams\n",
        "import os\n",
        "\n",
        "MODEL_ID = \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "\n",
        "tp = int(os.getenv(\"TP_SIZE\", \"1\"))  # set >1 if you want tensor parallel across GPUs\n",
        "\n",
        "llm = LLM(\n",
        "    model=MODEL_ID,\n",
        "    dtype=\"float16\",\n",
        "    trust_remote_code=True,\n",
        "    tensor_parallel_size=tp,\n",
        "    max_num_seqs=64,\n",
        "    max_model_len=131072,\n",
        "    mamba_ssm_cache_dtype=\"float32\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b4ed929-eaf2-4f55-999b-09153059af67",
      "metadata": {
        "id": "6b4ed929-eaf2-4f55-999b-09153059af67"
      },
      "source": [
        "### Sampling parameters\n",
        "\n",
        "When generating text, the model can either:\n",
        "\n",
        "- **Always pick the highest-probability token** (greedy decoding — fast and deterministic).\n",
        "- **Sample from the probability distribution** over possible next tokens — which adds variety and creativity.\n",
        "\n",
        "We use the following parameters to control that behavior:\n",
        "\n",
        "- `temperature = 0.3`: Controls randomness. Lower values → more confident, deterministic outputs. Higher values → more diverse, sometimes erratic responses.\n",
        "- `top_p = 0.9`: Enables **nucleus sampling** — the model samples only from the top tokens that together make up 90% of the probability mass. Balances diversity and coherence.\n",
        "- `max_tokens=512`: Controls maximum number of tokens to generate.\n",
        "\n",
        "When running experiments, you can try changing the prompts and sampling values below to see how the model’s behavior changes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "018c9576-1b59-4c1e-a87a-ec3decb7ca08",
      "metadata": {
        "id": "018c9576-1b59-4c1e-a87a-ec3decb7ca08"
      },
      "outputs": [],
      "source": [
        "sampling = SamplingParams(\n",
        "    max_tokens=512, temperature=0.3, top_p=0.9\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9bcd299-8b1a-4185-b6bb-b0898d195fc7",
      "metadata": {
        "id": "b9bcd299-8b1a-4185-b6bb-b0898d195fc7"
      },
      "source": [
        "### Convertng user prompts to chat\n",
        "\n",
        "Modern LLMs often use role-tagged messages to structure conversations, where each entry specifies both a role (such as `system`, `user`, or `assistant`) and the `content` of the message. This format helps the model distinguish between instructions, user inputs, and its own prior outputs, enabling more consistent and controllable behavior. The system role typically sets context or constraints (e.g., tone, style, or reasoning mode), the user role provides queries or prompts, and the assistant role represents the model’s responses. By encoding dialogue this way, developers can guide multi-turn interactions, support features like reasoning toggles, and make model behavior easier to align across different applications.\n",
        "\n",
        "The following helper function will be used to convert a list of role-tagged messages (e.g., system, user) into the exact chat prompt format expected by the **Nemotron Nano 9b v2 model**. It uses the tokenizer’s built-in chat template to render the conversation into text, appends a reasoning marker so the model knows it should reason or not, and outputs the result as a PyTorch tensor ready for inference.\n",
        "\n",
        "In this fundamentals course, we will keep reasoning off for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3faaf21-9ab0-4ce4-bc13-9bcb6f7c4a79",
      "metadata": {
        "id": "c3faaf21-9ab0-4ce4-bc13-9bcb6f7c4a79"
      },
      "outputs": [],
      "source": [
        "def to_chat(messages):\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": \"/no_think\"},   # or \"/think\"\n",
        "      {\"role\": \"user\",   \"content\": \"Write a haiku about GPUs\"}\n",
        "    ]\n",
        "    \"\"\"\n",
        "    return tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb1e93d-765a-444b-a2d5-15fb32cfd46a",
      "metadata": {
        "id": "edb1e93d-765a-444b-a2d5-15fb32cfd46a"
      },
      "source": [
        "## Executing inference with Nemotron Nano 9b v2\n",
        "\n",
        "Now that the model is loaded, let’s run an inference example. Here we hardcode the user’s question — “Explain KV cache in one paragraph.” — inside the message list, along with a system instruction that disables reasoning (/no_think). When executed, the model will process the chat prompt and return its response, which we print below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a79d1243-856a-48d0-b2fb-92c6281643a5",
      "metadata": {
        "id": "a79d1243-856a-48d0-b2fb-92c6281643a5"
      },
      "outputs": [],
      "source": [
        "# Chat prompt with reasoning OFF\n",
        "messages = [\n",
        "    {\"role\":\"system\", \"content\": \"/no_think\"},\n",
        "    {\"role\":\"user\",   \"content\": \"Explain KV cache in one paragraph.\"}\n",
        "]\n",
        "chat_prompt = to_chat(messages)\n",
        "\n",
        "outs = llm.generate([chat_prompt], sampling)\n",
        "for i, o in enumerate(outs, 1):\n",
        "    print(f\"=== Output {i} ===\\n{o.outputs[0].text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94030624-e454-4319-8b87-a4cc6033a170",
      "metadata": {
        "id": "94030624-e454-4319-8b87-a4cc6033a170"
      },
      "source": [
        "## Performance experiments\n",
        "\n",
        "Now, we explore how inference performance is evaluated. We’ll start by establishing a baseline, then run experiments across batch sizes and other parameters, and finally interpret the results with visualizations to understand the trade-offs between throughput, latency, and user experience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e926cc0-3fc6-4757-a233-0447d4fc5d1c",
      "metadata": {
        "id": "0e926cc0-3fc6-4757-a233-0447d4fc5d1c"
      },
      "source": [
        "### Helper functions\n",
        "\n",
        "These helper functions streamline benchmarking with Nemotron Nano-9B-v2.\n",
        "\n",
        "- `to_chat_no_think`: wraps a user string into the model’s chat template with reasoning explicitly disabled (`/no_think`), producing a prompt ready for inference.\n",
        "- `timed_generate`: runs generation while measuring elapsed time, token counts, and outputs. It supports batching, suppresses logs for clean runs, and returns both performance metrics and model responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3149c7f0-7c42-44dc-ba2e-6a94d9e9c9d2",
      "metadata": {
        "id": "3149c7f0-7c42-44dc-ba2e-6a94d9e9c9d2"
      },
      "outputs": [],
      "source": [
        "# Nemotron Nano-9B-v2 benchmarking helpers (reasoning OFF)\n",
        "# --- Imports & reproducibility ---\n",
        "import os, time, random, math, json, itertools, statistics, gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import nest_asyncio; nest_asyncio.apply()\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from vllm import SamplingParams  # llm must already exist in your session\n",
        "\n",
        "import os, logging\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\"                 # tqdm\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\" # huggingface-hub\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"   # Transformers text logs\n",
        "\n",
        "# Optional: quiet vLLM info logs too\n",
        "logging.getLogger(\"vllm\").setLevel(logging.ERROR)\n",
        "try:\n",
        "    from transformers.utils.logging import set_verbosity_error\n",
        "    set_verbosity_error()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "def to_chat_no_think(user_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Format a single-turn chat with reasoning explicitly OFF\n",
        "    using the model's chat template.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "        {\"role\": \"user\",   \"content\": user_text},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "# --- Timed generation with optional batching ---\n",
        "import io, contextlib\n",
        "\n",
        "def timed_generate(prompts, sampling_params, as_chat=True, return_counts=True, quiet=True):\n",
        "    \"\"\"Return (elapsed_s, total_gen_tokens, [counts], outputs).\"\"\"\n",
        "    if isinstance(prompts, str):\n",
        "        prompts = [prompts]\n",
        "\n",
        "    wrapped = [to_chat_no_think(p) for p in prompts] if as_chat else prompts\n",
        "\n",
        "    # define the actual call with tqdm disabled\n",
        "    def _call():\n",
        "        return llm.generate(wrapped, sampling_params, use_tqdm=False)\n",
        "\n",
        "    # time + suppress stdout/stderr if quiet\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    with torch.inference_mode():\n",
        "        if quiet:\n",
        "            with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
        "                outputs = _call()\n",
        "        else:\n",
        "            outputs = _call()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    elapsed = time.perf_counter() - t0\n",
        "\n",
        "    # per-sequence counts\n",
        "    counts = []\n",
        "    for o in outputs:\n",
        "        cand = o.outputs[0]\n",
        "        if getattr(cand, \"token_ids\", None) is not None:\n",
        "            cnt = len(cand.token_ids)\n",
        "        elif getattr(cand, \"token_count\", None) is not None:\n",
        "            cnt = int(cand.token_count)\n",
        "        else:\n",
        "            cnt = len(tokenizer.encode(cand.text, add_special_tokens=False))\n",
        "        counts.append(cnt)\n",
        "\n",
        "    total = sum(counts)\n",
        "    if return_counts:\n",
        "        return elapsed, total, counts, outputs\n",
        "    return elapsed, total, outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "488963f5-0e20-4299-b4a9-2511b8fec342",
      "metadata": {
        "id": "488963f5-0e20-4299-b4a9-2511b8fec342"
      },
      "source": [
        "### Baseline latency and throughput\n",
        "\n",
        "Let’s establish a **baseline** for how our model performs with a single prompt under typical sampling settings.\n",
        "\n",
        "This test measures:\n",
        "\n",
        "- **Latency**: the total time it takes to generate a complete response\n",
        "- **Throughput**: the number of tokens generated per second\n",
        "\n",
        "What we’re doing here:\n",
        "\n",
        "- Use a **single input prompt** (feel free to change it!)\n",
        "- Generate up to `256` tokens using greedy sampling (`temperature=0.0`, `top_p=1.0`)\n",
        "- Call `timed_generate()` to:\n",
        "  - time the entire inference run\n",
        "  - count how many tokens were actually generated\n",
        "- Print the generated response\n",
        "\n",
        "> This is the simplest “real-world” case: one user, one prompt, one answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e628f08-d4d9-4371-8972-bac2f357532a",
      "metadata": {
        "id": "2e628f08-d4d9-4371-8972-bac2f357532a"
      },
      "outputs": [],
      "source": [
        "# Greedy for latency/throughput measurement\n",
        "greedy = SamplingParams(max_tokens=256, temperature=0.0, top_p=1.0)\n",
        "\n",
        "batch = [\n",
        "    \"The future of AI is\"\n",
        "]\n",
        "\n",
        "elapsed, gen_toks, counts, outs  = timed_generate(batch, greedy, as_chat=True, return_counts=True)\n",
        "tok_s = gen_toks / elapsed if elapsed > 0 else float(\"nan\")\n",
        "print(f\"Requests: {len(batch)} | Gen tokens: {gen_toks} | Time: {elapsed:.2f}s | {tok_s:.1f} tok/s\")\n",
        "\n",
        "# Peek at first few outputs\n",
        "for i, o in enumerate(outs[:3], 1):\n",
        "    print(f\"\\n[{i}] {o.outputs[0].text.strip()[:600]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0e8d3e9-b348-49eb-b289-320f2e241002",
      "metadata": {
        "id": "a0e8d3e9-b348-49eb-b289-320f2e241002"
      },
      "source": [
        "### Batch size sweep\n",
        "\n",
        "Now we’re going to benchmark how inference performance changes with **different batch sizes** — that is, how many prompts we process in a single inference.\n",
        "\n",
        "This experiment helps us understand the **tradeoff between throughput and latency**, and find the batch size that balances performance and responsiveness.\n",
        "\n",
        "For each batch size in:\n",
        "\n",
        "```python\n",
        "[1, 8, 32, 64, 128, 256]\n",
        "```\n",
        "\n",
        "we run several repetitions and record:\n",
        "\n",
        "**Sustained throughput (capacity) in token/sec**\n",
        "\n",
        "- *What:* The average number of tokens the model generates per second across all requests in a batch.\n",
        "\n",
        "- *Why:* This measures the system’s capacity — how much output it can produce under steady load. High sustained throughput indicates efficient GPU utilization.\n",
        "\n",
        "- *How:* Divide the total number of generated tokens by the total wall-clock time of the run.\n",
        "\n",
        "**p50 / p90 total latency (sec)**\n",
        "\n",
        "- *What:* The end-to-end time a request takes, from submission until the final token is produced. Reported at median (p50) and tail (p90) percentiles.\n",
        "\n",
        "- *Why:* Total latency reflects the user’s wait for a complete answer. Percentiles show both typical experience (p50) and slower, less common cases (p90).\n",
        "\n",
        "- *How:* Measure completion time for each request, then compute percentiles.\n",
        "\n",
        "**p50 / p90 Time to First Token (TTFT) (sec)**\n",
        "\n",
        "- *What:* The delay before the model outputs its very first token, again reported at p50 and p90.\n",
        "\n",
        "- *Why:* TTFT captures perceived responsiveness — users notice when nothing appears on screen. Lower TTFT improves interactivity.\n",
        "\n",
        "- *How:* Record the elapsed time between request submission and the first token arriving, then compute percentiles.\n",
        "\n",
        "**p50 / p90 Inter-Token Latency (ITL) (sec/token)**\n",
        "\n",
        "- *What:* The average time gap between consecutive tokens after the first one, reported at p50 and p90.\n",
        "\n",
        "- *Why:* ITL reflects the speed of the autoregressive decode loop — critical for long outputs and interactive generation.\n",
        "\n",
        "- *How:* Measure the interval between successive tokens in a response and summarize with percentiles.\n",
        "\n",
        "**p50 / p90 throughput (from sec/token)**\n",
        "\n",
        "- *What:* The inverse of inter-token latency, reported at p50 and p90. Expresses how many tokens per second are produced within a single sequence.\n",
        "\n",
        "- *Why:* Per-sequence throughput highlights decoding efficiency and shows how well generation scales across requests.\n",
        "\n",
        "- *How:* Measure per-token delays within a sequence, take their reciprocal, and summarize with percentiles.\n",
        "\n",
        "#### Note on p50 and p90\n",
        "\n",
        "When evaluating inference performance, we usually report both **p50** (median) and **p90** (tail) instead of just a single number. The median (p50) reflects the “typical” user experience, while p90 highlights performance in slower cases that still happen frequently enough to matter for real workloads. Together, they give a balanced view of central tendency and variability.\n",
        "\n",
        "We generally avoid using **p99** in benchmarking tutorials because it is highly sensitive to outliers — one noisy run, system hiccup, or background process can dominate the statistic and make results misleading. In production, p99 can be useful for stress-testing service-level agreements (SLAs), but for model performance exploration, p50 and p90 are more stable and interpretable.\n",
        "\n",
        "#### Batch scheduling\n",
        "\n",
        "To ensure fair and repeatable benchmarking, we pre-generate a schedule of prompts for each batch size and run. Without scheduling, each run could randomly draw different prompts, introducing variation that makes it harder to compare results. By fixing a pool of prompts and assigning them consistently across runs, we isolate the effect of batch size and system behavior from the randomness of prompt content. This also mimics realistic serving conditions, where a system handles heterogeneous requests in batches, allowing us to study how throughput and latency scale under mixed workloads.\n",
        "\n",
        "#### Choice of sampling parameters\n",
        "\n",
        "For Nemotron Nano-9B-v2, the recommended setting is greedy decoding for non-reasoning mode (always picking the highest-probability token). However, in our experiments we deliberately introduced some randomness (e.g., using temperature or top-p sampling). This was useful for two reasons: (1) it better reflects real deployment scenarios, where generation often balances determinism with diversity, and (2) it helps stress-test performance by ensuring the model explores different decoding paths, rather than repeatedly generating the exact same tokens.\n",
        "\n",
        "#### Let's execute the benchmark!\n",
        "\n",
        "Now, as we covered all basics, let's execite the following cell.\n",
        "\n",
        "> These runs can take a few minutes, especially at large batch sizes — start executing the cell and grab a coffee!\n",
        "\n",
        "We will be saving the results in the file `batch_benchmark.csv` so you can reuse or visualize the data later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cfae0d7-5231-4c84-a3aa-8c4cefd0d96a",
      "metadata": {
        "id": "2cfae0d7-5231-4c84-a3aa-8c4cefd0d96a"
      },
      "outputs": [],
      "source": [
        "# Nemotron Nano-9B-v2 batch benchmark (reasoning OFF)\n",
        "# Uses a repeatable randomized prompt schedule per (batch size, run).\n",
        "\n",
        "# -------------------- Config --------------------\n",
        "BATCH_SIZES       = [1, 8, 32, 64, 128, 256]\n",
        "RUNS_PER_SIZE     = 10\n",
        "WARMUP_RUNS       = 2                       # drop warmups (JIT/capture)\n",
        "MAX_TOKENS_FULL   = 128\n",
        "USE_GREEDY        = False                   # False => non-greedy (realistic)\n",
        "TEMPERATURE       = 0.7\n",
        "TOP_P             = 0.9\n",
        "TOP_K             = 0\n",
        "REPETITION_PENALTY= 1.05\n",
        "SEED_SCHEDULE     = 2025                    # makes the random prompt schedule repeatable\n",
        "\n",
        "PROMPT_POOL = [\n",
        "    \"The future of AI is\",\n",
        "    \"Summarize in 1–2 sentences: why batching helps decode less than prefill.\",\n",
        "    \"Give 5 bullets on KV cache eviction strategies.\",\n",
        "    \"Explain attention vs. Mamba-2 in 3 concise points.\",\n",
        "    \"Write a 2-sentence TL;DR on FP16 vs. BF16 tradeoffs.\",\n",
        "    \"List 3 pitfalls when benchmarking LLMs.\"\n",
        "]\n",
        "\n",
        "# -------------------- Utilities --------------------\n",
        "def pct(vals, p, method=\"higher\"):\n",
        "    \"\"\"Nearest-rank-ish percentile on observed values (robust for small N).\"\"\"\n",
        "    arr = np.asarray(vals, float)\n",
        "    arr = arr[~np.isnan(arr)]\n",
        "    if arr.size == 0:\n",
        "        return float(\"nan\")\n",
        "    return float(np.percentile(arr, p, method=method))\n",
        "\n",
        "def build_prompt_schedule(bs_list, runs_total, pool, seed=SEED_SCHEDULE):\n",
        "    \"\"\"Pre-generate a repeatable prompt batch for each (bs, run).\"\"\"\n",
        "    rng = random.Random(seed)\n",
        "    sched = {bs: [] for bs in bs_list}\n",
        "    for bs in bs_list:\n",
        "        for _ in range(runs_total):\n",
        "            sched[bs].append([rng.choice(pool) for _ in range(bs)])\n",
        "    return sched\n",
        "\n",
        "SCHEDULE = build_prompt_schedule(BATCH_SIZES, RUNS_PER_SIZE + WARMUP_RUNS, PROMPT_POOL)\n",
        "\n",
        "# Sampling options\n",
        "if USE_GREEDY:\n",
        "    SAMPLING_OPTS = dict(temperature=0.0, top_p=1.0)\n",
        "else:\n",
        "    SAMPLING_OPTS = dict(temperature=TEMPERATURE, top_p=TOP_P, top_k=TOP_K,\n",
        "                         repetition_penalty=REPETITION_PENALTY, seed=1234)\n",
        "\n",
        "# -------------------- Benchmark --------------------\n",
        "records = []\n",
        "\n",
        "for bs in BATCH_SIZES:\n",
        "    print(f\"Processing with the batch size (BS) = {bs:>3} ...\")\n",
        "\n",
        "    # Per-run logs\n",
        "    first_latencies = []            # seconds\n",
        "    total_latencies = []            # seconds\n",
        "    sec_per_token_runs = []         # seconds / token\n",
        "    itl_sec_per_token_runs = []     # seconds / token (beyond first tokens)\n",
        "\n",
        "    # Aggregates for sustained throughput\n",
        "    sum_tokens_total = 0\n",
        "    sum_elapsed_total = 0.0\n",
        "\n",
        "    for r in range(RUNS_PER_SIZE + WARMUP_RUNS):\n",
        "        prompts = SCHEDULE[bs][r]\n",
        "\n",
        "        # TTFT (first token only)\n",
        "        t_first, _, _, _ = timed_generate(\n",
        "            prompts,\n",
        "            SamplingParams(**SAMPLING_OPTS, max_tokens=1),\n",
        "            as_chat=True,\n",
        "            return_counts=True\n",
        "        )\n",
        "\n",
        "        # Full answer\n",
        "        t_total, gen_tokens, counts, outs = timed_generate(\n",
        "            prompts,\n",
        "            SamplingParams(**SAMPLING_OPTS, max_tokens=MAX_TOKENS_FULL),\n",
        "            as_chat=True,\n",
        "            return_counts=True\n",
        "        )\n",
        "\n",
        "        # Warmup discard\n",
        "        if r < WARMUP_RUNS:\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            continue\n",
        "\n",
        "        # Track latencies\n",
        "        first_latencies.append(t_first)\n",
        "        total_latencies.append(t_total)\n",
        "\n",
        "        # Sustained throughput aggregates\n",
        "        sum_tokens_total += gen_tokens\n",
        "        sum_elapsed_total += t_total\n",
        "\n",
        "        # Throughput percentiles (via sec/token)\n",
        "        if gen_tokens > 0:\n",
        "            sec_per_token_runs.append(t_total / gen_tokens)\n",
        "\n",
        "        # ITL: exclude one first token per *active* sequence\n",
        "        post_first_tokens = sum((c - 1) for c in counts if c > 0)\n",
        "        itl_den = max(post_first_tokens, 1)\n",
        "        itl_sec_per_token_runs.append(max(t_total - t_first, 0.0) / itl_den)\n",
        "\n",
        "        gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    # --- Metrics ---\n",
        "    p50_ttft_s    = pct(first_latencies, 50)\n",
        "    p90_ttft_s    = pct(first_latencies, 90)\n",
        "    p50_latency_s = pct(total_latencies, 50)\n",
        "    p90_latency_s = pct(total_latencies, 90)\n",
        "\n",
        "    throughput_sustained_tok_s = (sum_tokens_total / sum_elapsed_total) if sum_elapsed_total > 0 else float(\"nan\")\n",
        "\n",
        "    p50_thru_tok_s = (1.0 / pct(sec_per_token_runs, 50)) if sec_per_token_runs else float(\"nan\")\n",
        "    p90_thru_tok_s = (1.0 / pct(sec_per_token_runs, 90)) if sec_per_token_runs else float(\"nan\")\n",
        "\n",
        "    itl_p50_ms_per_tok = 1000.0 * pct(itl_sec_per_token_runs, 50) if itl_sec_per_token_runs else float(\"nan\")\n",
        "    itl_p90_ms_per_tok = 1000.0 * pct(itl_sec_per_token_runs, 90) if itl_sec_per_token_runs else float(\"nan\")\n",
        "\n",
        "    records.append(dict(\n",
        "        batch_size                 = bs,\n",
        "        p50_ttft_s                 = p50_ttft_s,\n",
        "        p90_ttft_s                 = p90_ttft_s,\n",
        "        p50_latency_s              = p50_latency_s,\n",
        "        p90_latency_s              = p90_latency_s,\n",
        "        throughput_sustained_tok_s = throughput_sustained_tok_s,\n",
        "        throughput_p50_tok_s       = p50_thru_tok_s,\n",
        "        throughput_p90_tok_s       = p90_thru_tok_s,\n",
        "        itl_p50_ms_per_tok         = itl_p50_ms_per_tok,\n",
        "        itl_p90_ms_per_tok         = itl_p90_ms_per_tok,\n",
        "        runs                       = RUNS_PER_SIZE\n",
        "    ))\n",
        "\n",
        "    print(\n",
        "        f\"BS={bs:>3} | p50_lat={p50_latency_s:.3f}s | p90_lat={p90_latency_s:.3f}s | \"\n",
        "        f\"sustained={throughput_sustained_tok_s:.1f} tok/s | p50_thru={p50_thru_tok_s:.1f} | p90_thru={p90_thru_tok_s:.1f} | \"\n",
        "        f\"ITL p50={itl_p50_ms_per_tok:.1f} ms/tok | ITL p90={itl_p90_ms_per_tok:.1f} ms/tok\"\n",
        "    )\n",
        "\n",
        "df = pd.DataFrame.from_records(records).sort_values(\"batch_size\")\n",
        "display(df)\n",
        "\n",
        "df.to_csv(\"batch_benchmark.csv\", index=False)\n",
        "print(\"✅ Benchmark finished – saved to batch_benchmark.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82e73fdd-59a8-4d27-9895-d36383c073bb",
      "metadata": {
        "id": "82e73fdd-59a8-4d27-9895-d36383c073bb"
      },
      "source": [
        "### Interpreting our benchmarks\n",
        "\n",
        "Now that we've run our performance sweep, let’s dive into what the results actually mean — and what insights we can extract from them. With the raw measurements in hand, we turn to visual analysis. Each plot focuses on a different aspect of model performance, helping us interpret the results in terms of real-world inference trade-offs.\n",
        "\n",
        "Let's load the benchmark file if you are returning to the notebook (ignore this step if you've just executed the benchmark)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74afcb06-d008-45f5-9f8c-4f73406d7610",
      "metadata": {
        "id": "74afcb06-d008-45f5-9f8c-4f73406d7610"
      },
      "outputs": [],
      "source": [
        "# Load df from memory if present, otherwise read the CSV; validate columns.\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "if \"df\" not in globals():\n",
        "    candidates = [\"batch_benchmark.csv\", \"/workspace/batch_benchmark.csv\"]\n",
        "    for p in candidates:\n",
        "        if os.path.exists(p):\n",
        "            df = pd.read_csv(p)\n",
        "            break\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Couldn't find batch_benchmark.csv. Run the benchmark first.\")\n",
        "\n",
        "# Ensure types and order\n",
        "df[\"batch_size\"] = df[\"batch_size\"].astype(int)\n",
        "df = df.sort_values(\"batch_size\").reset_index(drop=True)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e5edef5-ec3f-40ed-ad8a-6b462e827072",
      "metadata": {
        "id": "0e5edef5-ec3f-40ed-ad8a-6b462e827072"
      },
      "source": [
        "#### Helper for consistent plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13008f21-c7fd-4836-a4a7-09c585bf1c39",
      "metadata": {
        "id": "13008f21-c7fd-4836-a4a7-09c585bf1c39"
      },
      "outputs": [],
      "source": [
        "# Small helper to keep plots consistent (one chart per figure).\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_lines(x, series_list, title, xlabel, ylabel, ylog=False, xticks=None):\n",
        "    plt.figure()\n",
        "    for y, label, marker in series_list:\n",
        "        plt.plot(x, y, marker=marker, label=label)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
        "    if xticks is not None:\n",
        "        plt.xticks(xticks)\n",
        "    if ylog:\n",
        "        plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acdcc98e-8922-4546-b438-6f97a5d4b0af",
      "metadata": {
        "id": "acdcc98e-8922-4546-b438-6f97a5d4b0af"
      },
      "source": [
        "#### Throughput vs batch size (sustained, p50, p90)\n",
        "\n",
        "Shows how sustained throughput scales with batch size, including median (p50) and high-tail (p90) behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e80d0b5-fca4-4399-aa70-277dc2aeba2e",
      "metadata": {
        "id": "2e80d0b5-fca4-4399-aa70-277dc2aeba2e"
      },
      "outputs": [],
      "source": [
        "x = df[\"batch_size\"].tolist()\n",
        "\n",
        "plot_lines(\n",
        "    x,\n",
        "    [\n",
        "        (df[\"throughput_sustained_tok_s\"], \"Sustained\", \"o\"),\n",
        "        (df[\"throughput_p50_tok_s\"],       \"p50\",       \"s\"),\n",
        "        (df[\"throughput_p90_tok_s\"],       \"p90\",       \"^\"),\n",
        "    ],\n",
        "    title=\"Throughput vs Batch Size (tokens/sec)\",\n",
        "    xlabel=\"Batch size\",\n",
        "    ylabel=\"Tokens / second\",\n",
        "    ylog=False,\n",
        "    xticks=x\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c177a65-c248-470e-846e-85254107c8c2",
      "metadata": {
        "id": "3c177a65-c248-470e-846e-85254107c8c2"
      },
      "source": [
        "**Interpretation:** Throughput scales steeply with batch size up to around 64, after which it plateaus — indicating the GPU is fully saturated. Sustained, p50, and p90 values overlap closely, showing that throughput is stable and predictable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f553c7c4-dcf0-4db9-8fe1-9cffd0ea27ae",
      "metadata": {
        "id": "f553c7c4-dcf0-4db9-8fe1-9cffd0ea27ae"
      },
      "source": [
        "#### Total latency vs batch size (p50, p90)\n",
        "\n",
        "Illustrates how overall request latency grows as batch sizes increase, exposing potential bottlenecks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5b1946-b57b-4de6-927e-23bd9dac464b",
      "metadata": {
        "id": "ec5b1946-b57b-4de6-927e-23bd9dac464b"
      },
      "outputs": [],
      "source": [
        "x = df[\"batch_size\"].tolist()\n",
        "\n",
        "plot_lines(\n",
        "    x,\n",
        "    [\n",
        "        (df[\"p50_latency_s\"], \"p50 total latency\", \"o\"),\n",
        "        (df[\"p90_latency_s\"], \"p90 total latency\", \"s\"),\n",
        "    ],\n",
        "    title=\"Total Latency vs Batch Size\",\n",
        "    xlabel=\"Batch size\",\n",
        "    ylabel=\"Seconds\",\n",
        "    ylog=True,      # latencies can span orders of magnitude\n",
        "    xticks=x\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a97c6a49-84f1-476e-ab7a-a932a6dcfcad",
      "metadata": {
        "id": "a97c6a49-84f1-476e-ab7a-a932a6dcfcad"
      },
      "source": [
        "**Interpretation:** Total latency increases steadily with batch size — larger batches take significantly longer to complete, even though they improve throughput. The gap between p50 and p90 is very small, indicating stable performance across requests. This shows the trade-off: batching boosts capacity but comes at the cost of higher end-to-end wait times, which matters for user-facing workloads."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47a89e9a-75df-4f8d-a127-352e1a9fd5b7",
      "metadata": {
        "id": "47a89e9a-75df-4f8d-a127-352e1a9fd5b7"
      },
      "source": [
        "#### Time to First Token (TTFT) vs batch size (p50, p90)\n",
        "\n",
        "Focuses on responsiveness by measuring how quickly the first token is produced under different batch loads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785eeaf6-d7e9-4e0b-95c7-978e17619ec8",
      "metadata": {
        "id": "785eeaf6-d7e9-4e0b-95c7-978e17619ec8"
      },
      "outputs": [],
      "source": [
        "x = df[\"batch_size\"].tolist()\n",
        "\n",
        "plot_lines(\n",
        "    x,\n",
        "    [\n",
        "        (df[\"p50_ttft_s\"], \"p50 TTFT\", \"o\"),\n",
        "        (df[\"p90_ttft_s\"], \"p90 TTFT\", \"s\"),\n",
        "    ],\n",
        "    title=\"Time to First Token (TTFT) vs Batch Size\",\n",
        "    xlabel=\"Batch size\",\n",
        "    ylabel=\"Seconds\",\n",
        "    ylog=True,\n",
        "    xticks=x\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8099a562-6232-48f6-bb0d-47bacadbd893",
      "metadata": {
        "id": "8099a562-6232-48f6-bb0d-47bacadbd893"
      },
      "source": [
        "**Interpretation:** Time to First Token (TTFT) grows steadily with batch size, meaning responsiveness degrades as more requests are batched together. While small batches deliver near-instant feedback (<1s), larger ones (128–256) push TTFT into several seconds. The tight overlap of p50 and p90 shows the effect is consistent across requests, underscoring the trade-off between batching for throughput and keeping the system interactive."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c423ba58-b4c5-46c4-8b5f-10eb16461529",
      "metadata": {
        "id": "c423ba58-b4c5-46c4-8b5f-10eb16461529"
      },
      "source": [
        "#### First-token vs total latency across batch sizes (p50, p90)\n",
        "\n",
        "Compares initial responsiveness with full completion time, clarifying the trade-off between early feedback and overall latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7db988-9b2a-4ead-a1cd-277c7e31d5a6",
      "metadata": {
        "id": "5f7db988-9b2a-4ead-a1cd-277c7e31d5a6"
      },
      "outputs": [],
      "source": [
        "# Sort & extract\n",
        "x = df[\"batch_size\"].tolist()\n",
        "\n",
        "ttft_p50 = df[\"p50_ttft_s\"].tolist()\n",
        "ttft_p90 = df[\"p90_ttft_s\"].tolist()\n",
        "lat_p50  = df[\"p50_latency_s\"].tolist()\n",
        "lat_p90  = df[\"p90_latency_s\"].tolist()\n",
        "\n",
        "# Single figure with four series\n",
        "plt.figure()\n",
        "plt.plot(x, ttft_p50, marker=\"o\", label=\"TTFT (p50)\")\n",
        "plt.plot(x, ttft_p90, marker=\"^\", label=\"TTFT (p90)\")\n",
        "plt.plot(x, lat_p50,  marker=\"s\", label=\"Total latency (p50)\")\n",
        "plt.plot(x, lat_p90,  marker=\"x\", label=\"Total latency (p90)\")\n",
        "\n",
        "plt.title(\"First Token vs Total Latency — p50 & p90\")\n",
        "plt.xlabel(\"Batch size\")\n",
        "plt.ylabel(\"Seconds\")\n",
        "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
        "plt.xticks(x)\n",
        "plt.yscale(\"log\")  # latency scales fast with batch size; log makes it readable\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0427b8d8-2357-4cd9-b430-0e30f3ae68e4",
      "metadata": {
        "id": "0427b8d8-2357-4cd9-b430-0e30f3ae68e4"
      },
      "source": [
        "**Interpretation:** This chart shows how Time to First Token (TTFT) and total latency evolve together as batch size increases. TTFT remains much lower than total latency, meaning users see early feedback well before the full response finishes. However, both grow with larger batches, and the widening gap emphasizes the trade-off: batching improves throughput but stretches total completion times, while still preserving some interactivity through faster first token delivery."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c965f9a-745b-4abb-940b-d45d35495c83",
      "metadata": {
        "id": "2c965f9a-745b-4abb-940b-d45d35495c83"
      },
      "source": [
        "#### Inter-Token Latency (ITL) vs batch size (p50, p90)\n",
        "\n",
        "Looks at how long it takes to generate each additional token after the first, showing scaling behavior inside the decode loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bdcfac7-6528-4d35-a487-8c89c147c721",
      "metadata": {
        "id": "6bdcfac7-6528-4d35-a487-8c89c147c721"
      },
      "outputs": [],
      "source": [
        "x = df[\"batch_size\"].tolist()\n",
        "\n",
        "plot_lines(\n",
        "    x,\n",
        "    [\n",
        "        (df[\"itl_p50_ms_per_tok\"], \"ITL p50\", \"o\"),\n",
        "        (df[\"itl_p90_ms_per_tok\"], \"ITL p90\", \"s\"),\n",
        "    ],\n",
        "    title=\"Inter-Token Latency (ITL) vs Batch Size\",\n",
        "    xlabel=\"Batch size\",\n",
        "    ylabel=\"Milliseconds / token\",\n",
        "    ylog=True,\n",
        "    xticks=x\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ee8a6d-0d56-4f4e-b62d-1a7b802ddf16",
      "metadata": {
        "id": "e7ee8a6d-0d56-4f4e-b62d-1a7b802ddf16"
      },
      "source": [
        "**Interpretation:** Inter-Token Latency (ITL) drops sharply as batch size increases, stabilizing around 64 and beyond. This means once the first token is produced, subsequent tokens stream out much faster with larger batches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc4c4ee-cfdb-458c-9825-53b5a4f44be8",
      "metadata": {
        "id": "1cc4c4ee-cfdb-458c-9825-53b5a4f44be8"
      },
      "source": [
        "#### Throughput vs latency trade-off (capacity vs responsiveness)\n",
        "\n",
        "Highlights the fundamental trade-off: increasing batch size improves throughput but can hurt latency, affecting user experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7861ab6a-621d-459d-a767-a7921f4e6ae6",
      "metadata": {
        "id": "7861ab6a-621d-459d-a767-a7921f4e6ae6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = df[\"p50_latency_s\"]\n",
        "y = df[\"throughput_sustained_tok_s\"]\n",
        "labels = df[\"batch_size\"].tolist()\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x, y, marker=\"o\")\n",
        "for xi, yi, lbl in zip(x, y, labels):\n",
        "    plt.annotate(str(lbl), (xi, yi), textcoords=\"offset points\", xytext=(5,5))\n",
        "plt.title(\"Throughput vs p50 Latency\")\n",
        "plt.xlabel(\"p50 total latency (s)\")\n",
        "plt.ylabel(\"Sustained throughput (tok/s)\")\n",
        "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f33d6c25-8711-45c3-a417-14aacbbe1414",
      "metadata": {
        "id": "f33d6c25-8711-45c3-a417-14aacbbe1414"
      },
      "source": [
        "**Interpretation:** This chart highlights the throughput–latency trade-off. As batch size increases, throughput climbs sharply, but total latency also grows. The curve flattens beyond batch size 64, where throughput gains are small compared to the jump in latency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca9c555-984c-4c69-ac71-6eb94dc03156",
      "metadata": {
        "id": "2ca9c555-984c-4c69-ac71-6eb94dc03156"
      },
      "source": [
        "#### Latency vs Throughput with knee point (p50 latency vs sustained throughput)\n",
        "\n",
        "Identifies the “knee point” — the optimal balance where adding more requests no longer yields proportional throughput gains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "639c7219-e592-41ce-93c3-50e8f90c6895",
      "metadata": {
        "id": "639c7219-e592-41ce-93c3-50e8f90c6895"
      },
      "outputs": [],
      "source": [
        "L_COL = \"p50_latency_s\"              # latency column\n",
        "T_COL = \"throughput_sustained_tok_s\" # throughput column\n",
        "\n",
        "needed = [\"batch_size\", L_COL, T_COL]\n",
        "missing = [c for c in needed if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}. Re-run the benchmark with those fields.\")\n",
        "\n",
        "# Sort and extract arrays\n",
        "df = df.sort_values(\"batch_size\").reset_index(drop=True)\n",
        "bs  = df[\"batch_size\"].to_numpy(dtype=int)\n",
        "lat = df[L_COL].to_numpy(dtype=float)\n",
        "thr = df[T_COL].to_numpy(dtype=float)\n",
        "\n",
        "# Normalize latency to [0,1]; normalize throughput and invert to [0,1]\n",
        "lat_n   = lat / np.max(lat)                    # higher is worse\n",
        "thr_inv = 1.0 - (thr / np.max(thr))            # invert so higher is worse\n",
        "\n",
        "# Gap between the two normalized curves → knee where gap is smallest\n",
        "gap    = np.abs(lat_n - thr_inv)\n",
        "best_i = int(np.argmin(gap))\n",
        "\n",
        "# Plot in your requested style\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(bs, lat_n,   'o-', label=f'{L_COL} (norm)')\n",
        "plt.plot(bs, thr_inv, '^--', label=f'{T_COL} (inverted norm)')\n",
        "plt.scatter(bs[best_i], lat_n[best_i], c='red', s=120,\n",
        "            label=f'sweet spot ≈ BS {bs[best_i]}')\n",
        "plt.xlabel(\"Batch size\"); plt.ylabel(\"Normalised metric (0-1)\")\n",
        "plt.title(\"Latency vs Throughput – knee point\")\n",
        "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\n",
        "    f\"Knee ~ BS={bs[best_i]} | \"\n",
        "    f\"latency={lat[best_i]:.3f}s | throughput={thr[best_i]:.1f} tok/s\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a086caf5-1fb0-4159-914e-6681f9827bbb",
      "metadata": {
        "id": "a086caf5-1fb0-4159-914e-6681f9827bbb"
      },
      "source": [
        "**Interpretation:** This plot visualizes the knee point where increasing batch size no longer delivers meaningful throughput gains relative to the rise in latency. The sweet spot appears around batch size 32 — beyond this, throughput plateaus while latency continues to climb. This point represents the best balance between system capacity and responsiveness for most practical workloads."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc6bfe86-762c-41d6-a2d2-f75d50cfa023",
      "metadata": {
        "id": "dc6bfe86-762c-41d6-a2d2-f75d50cfa023"
      },
      "source": [
        "#### Finding the sweet spot based on UX budget\n",
        "\n",
        "Choosing the optimal batch size isn’t just about throughput — it’s about respecting your users’ **latency expectations**.\n",
        "\n",
        "This section helps you find the best-performing batch size that stays within a given **latency budget**, defined in seconds. You can adjust the value of `latency_budget` depending on your application’s needs (e.g. 2.5s for chat, 5s for summarization, etc.).\n",
        "\n",
        "What this code does:\n",
        "\n",
        "- Filters all benchmark results to include only configurations where latency is **below your target threshold**\n",
        "- Then selects the **batch size with the highest throughput** from the remaining options\n",
        "- Runs this selection twice:\n",
        "  - Once using **p50 latency** (typical case)\n",
        "  - Once using **p90 latency** (tail latency)\n",
        "\n",
        "Try changing the `latency_budget` value and observe how the recommended batch size shifts depending on the metric used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6793b4d4-177c-4ea1-a755-0ace9a779098",
      "metadata": {
        "id": "6793b4d4-177c-4ea1-a755-0ace9a779098"
      },
      "outputs": [],
      "source": [
        "latency_budget = 5  # seconds\n",
        "\n",
        "filtered_p50 = df[df[\"p50_latency_s\"] <= latency_budget]\n",
        "filtered_p90 = df[df[\"p90_latency_s\"] <= latency_budget]\n",
        "\n",
        "if not filtered_p50.empty:\n",
        "    best_row_p50 = filtered_p50.loc[filtered_p50[\"throughput_sustained_tok_s\"].idxmax()]\n",
        "    print(f\"✅ Recommended: batch_size={int(best_row_p50.batch_size)}  \"\n",
        "          f\"⇒  p50 latency = {best_row_p50.p50_latency_s:.2f}s, \"\n",
        "          f\"throughput = {best_row_p50.throughput_sustained_tok_s:.0f} tok/s\")\n",
        "else:\n",
        "    print(f\"❌ No configuration meets the latency budget of {latency_budget:.1f}s\")\n",
        "\n",
        "if not filtered_p90.empty:\n",
        "    best_row_p90 = filtered_p90.loc[filtered_p90[\"throughput_sustained_tok_s\"].idxmax()]\n",
        "    print(f\"✅ Recommended: batch_size={int(best_row_p90.batch_size)}  \"\n",
        "          f\"⇒  p90 latency = {best_row_p90.p90_latency_s:.2f}s, \"\n",
        "          f\"throughput = {best_row_p90.throughput_sustained_tok_s:.0f} tok/s\")\n",
        "else:\n",
        "    print(f\"❌ No configuration meets the latency budget of {latency_budget:.1f}s based on p90 latency\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57695205-5a25-4257-b63a-79fb7548b584",
      "metadata": {
        "id": "57695205-5a25-4257-b63a-79fb7548b584"
      },
      "source": [
        "### Asessing impact of other parameters\n",
        "\n",
        "So far, we’ve seen that **batch size** directly affects latency and throughput. But batch size isn’t the only factor that matters.\n",
        "\n",
        "Let’s now explore how **other inference parameters**, starting with **output sequence length**, influence performance.\n",
        "\n",
        "#### Sequence Length\n",
        "\n",
        "The **sequence length** `max_tokens` parameter defines how many tokens the model is allowed to generate per prompt.\n",
        "\n",
        "In practice:\n",
        "\n",
        "- Short outputs (e.g. 32 tokens) return quickly\n",
        "- Long outputs (e.g. 512 tokens) take significantly more time, especially at high batch sizes\n",
        "\n",
        "This is because while **prefill cost is fixed**, the **decode phase scales linearly** with the number of tokens generated — we will look into that in future tutorials!\n",
        "\n",
        "What this experiment does:\n",
        "\n",
        "- Uses a fixed batch size (`batch = 32`)\n",
        "- Varies the `max_tokens` cap from 32 to 256\n",
        "- Measures **mean** and **p90 latency** across 20 runs for each setting\n",
        "\n",
        "> Note: This experiment can take a few minutes — longer sequence lengths at high batch size are compute-intensive.\n",
        "\n",
        "Let’s visualize how increasing the output length impacts both average latency and tail latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add23d47-4411-445d-8406-faaee29c4f35",
      "metadata": {
        "id": "add23d47-4411-445d-8406-faaee29c4f35"
      },
      "outputs": [],
      "source": [
        "# Latency vs Generated Length (p50 & p90) — batch fixed\n",
        "\n",
        "import gc, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from vllm import SamplingParams\n",
        "\n",
        "seq_lengths = [32, 64, 128, 256]\n",
        "BATCH       = 32\n",
        "RUNS        = 20   # effective samples per point (after warmup)\n",
        "WARMUP      = 2\n",
        "\n",
        "# Greedy sampling\n",
        "GREEDY = dict(temperature=0.0, top_p=1.0)\n",
        "\n",
        "lat_p50s, lat_p90s = [], []\n",
        "\n",
        "for mt in seq_lengths:\n",
        "    print(f\"Processing with the sequence length = {mt:>3} ...\")\n",
        "    sp = SamplingParams(max_tokens=mt, **GREEDY)\n",
        "    lats = []\n",
        "    for r in range(RUNS + WARMUP):\n",
        "        prompts = [\"AI future is\"] * BATCH\n",
        "        t_total, _, _, _ = timed_generate(prompts, sp, as_chat=True, return_counts=True)\n",
        "        if r < WARMUP:\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            continue\n",
        "        lats.append(t_total)\n",
        "        gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    # Percentiles on observed latencies (nearest-rank style)\n",
        "    p50 = float(np.percentile(lats, 50, method=\"higher\")) if lats else float(\"nan\")\n",
        "    p90 = float(np.percentile(lats, 90, method=\"higher\")) if lats else float(\"nan\")\n",
        "    lat_p50s.append(p50)\n",
        "    lat_p90s.append(p90)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(seq_lengths, lat_p50s, marker='o', label='p50 latency')\n",
        "plt.plot(seq_lengths, lat_p90s, marker='s', label='p90 latency')\n",
        "plt.xlabel('max_tokens'); plt.ylabel('Latency (s)'); plt.title('Latency vs Generated Length (Batch = 32)')\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.legend(); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a496db-e1d4-4f7c-9fed-9d99bf0f9ea1",
      "metadata": {
        "id": "a5a496db-e1d4-4f7c-9fed-9d99bf0f9ea1"
      },
      "source": [
        "**Interpretation:** Latency increases linearly with generated length, as expected in autoregressive decoding. This confirms that longer outputs scale predictably in cost, making latency primarily a function of sequence length rather than variability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1f9f29a-1367-42fe-b446-8d2d4f1d293e",
      "metadata": {
        "id": "e1f9f29a-1367-42fe-b446-8d2d4f1d293e"
      },
      "source": [
        "#### Sampling knobs (temperature & top-p) and determinism\n",
        "\n",
        "You might be wondering — do **sampling parameters** like `temperature` and `top_p` affect latency? Let's check!\n",
        "\n",
        "We run multiple generations using different combinations of:\n",
        "\n",
        "- `temperature = [0.0, 0.7, 1.3]`\n",
        "- `top_p     = [0.8, 0.9, 1.0]`\n",
        "\n",
        "For each pair, we measure:\n",
        "\n",
        "- **Mean latency**\n",
        "- **p90 latency**\n",
        "\n",
        "This helps us assess whether sampling diversity impacts performance.\n",
        "\n",
        "Let’s visualize the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ca64de-9bd5-41d3-8f5e-6eb4b439afc3",
      "metadata": {
        "id": "42ca64de-9bd5-41d3-8f5e-6eb4b439afc3"
      },
      "outputs": [],
      "source": [
        "# Latency vs Sampling Parameters — p50 & p90 (Nemotron Nano-9B-v2, reasoning OFF)\n",
        "\n",
        "import gc, numpy as np, itertools, matplotlib.pyplot as plt\n",
        "from vllm import SamplingParams\n",
        "\n",
        "# ----- Grid to sweep -----\n",
        "temps = [0.0, 0.7, 1.3]\n",
        "tops  = [0.8, 0.9, 1.0]\n",
        "grid  = list(itertools.product(temps, tops))\n",
        "\n",
        "# ----- Benchmark knobs -----\n",
        "BATCH        = 16          # run >1 to exercise scheduler\n",
        "RUNS         = 20          # effective samples per point (after warmup)\n",
        "WARMUP       = 2\n",
        "MAX_TOKENS   = 64\n",
        "SEED         = 1234        # set for reproducibility; set to None for true randomness\n",
        "\n",
        "def pct(vals, p, method=\"higher\"):\n",
        "    \"\"\"Nearest-rank-ish percentile using observed values.\"\"\"\n",
        "    arr = np.asarray(vals, float)\n",
        "    arr = arr[~np.isnan(arr)]\n",
        "    if arr.size == 0:\n",
        "        return float(\"nan\")\n",
        "    return float(np.percentile(arr, p, method=method))\n",
        "\n",
        "# ----- Results -----\n",
        "lat_p50s, lat_p90s = [], []\n",
        "labels = []\n",
        "\n",
        "for t, p in grid:\n",
        "    print(f\"Running with T = {t:<3} top_p = {p:<3} ...\")\n",
        "\n",
        "    sp = SamplingParams(\n",
        "        temperature=t,\n",
        "        top_p=p,\n",
        "        max_tokens=MAX_TOKENS\n",
        "    )\n",
        "\n",
        "    lats = []\n",
        "    for r in range(RUNS + WARMUP):\n",
        "        prompts = [\"The capital of France is\"] * BATCH\n",
        "        t_total, _, _, _ = timed_generate(prompts, sp, as_chat=True, return_counts=True)\n",
        "        if r < WARMUP:\n",
        "            gc.collect(); import torch; torch.cuda.empty_cache()\n",
        "            continue\n",
        "        lats.append(t_total)\n",
        "        gc.collect(); import torch; torch.cuda.empty_cache()\n",
        "\n",
        "    p50 = pct(lats, 50)\n",
        "    p90 = pct(lats, 90)\n",
        "    lat_p50s.append(p50)\n",
        "    lat_p90s.append(p90)\n",
        "    labels.append(f\"T={t}, p={p}\")\n",
        "\n",
        "    print(f\"T={t:<3} top_p={p:<3} | p50 {p50:.3f}s | p90 {p90:.3f}s\")\n",
        "\n",
        "# ----- Visualization -----\n",
        "x = range(len(grid))\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(x, lat_p50s, marker='o', label='p50 latency')\n",
        "plt.plot(x, lat_p90s, marker='s', label='p90 latency')\n",
        "plt.xticks(x, labels, rotation=45, ha='right')\n",
        "plt.xlabel('(temperature, top_p)')\n",
        "plt.ylabel('Latency (s)')\n",
        "plt.title(f'Latency vs Sampling Parameters (batch={BATCH}, max_tokens={MAX_TOKENS})')\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "265904c7-76e6-46fc-beb3-c592197bd5b7",
      "metadata": {
        "id": "265904c7-76e6-46fc-beb3-c592197bd5b7"
      },
      "source": [
        "**Interpretation:** Latency is largely unaffected by typical sampling parameter choices (temperature, top-p), remaining stable across most settings. Only at the extreme (high temperature with unrestricted top-p) does p90 latency spike, showing that aggressive randomness can introduce extra variability. This confirms that decoding strategies influence output quality more than raw performance, except in edge cases."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db04346-de3e-4fe2-ae57-140749f5fb17",
      "metadata": {
        "id": "1db04346-de3e-4fe2-ae57-140749f5fb17"
      },
      "source": [
        "## Note on reasoning models\n",
        "\n",
        "While this tutorial focuses on explaining LLM inference and its performance, it's important to recognize that not all language models are built the same.\n",
        "\n",
        "Some are optimized for **throughput and deployment efficiency** (e.g. Mistral 7B, LLaMA 3 8B), while others are designed for **complex reasoning and instruction following** (e.g. GPT-4, Claude Opus, Gemini Pro, LLaMA 3 70B, NVIDIA Llama Nemotron).\n",
        "\n",
        "When working with reasoning models, it's important to evaluate them not just on latency and throughput, but also on:\n",
        "\n",
        "- Accuracy and depth of reasoning\n",
        "- Consistency across temperature settings\n",
        "- Ability to follow multi-step or long-form instructions\n",
        "\n",
        "That said, **UX rules still apply**: a powerful model that takes 10 seconds to respond without streaming may still feel unusable, no matter how smart it is.\n",
        "\n",
        "> In production, you'll often need to balance **speed** with **output quality** — especially for use cases involving multi-step reasoning or decision making.\n",
        "\n",
        "👉 **Choose the model based on the application**: chat assistants, real-time agents, RAG pipelines, and batch summarization all have different performance vs. quality requirements.\n",
        "\n",
        "The right tradeoff isn't about raw metrics — it's about delivering the right user experience for the task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a5b8f41-a5b3-48bf-a844-1aac5b22ed87",
      "metadata": {
        "id": "0a5b8f41-a5b3-48bf-a844-1aac5b22ed87"
      },
      "source": [
        "## Stop this notebook\n",
        "\n",
        "Make sure to restart the jupyter kernel before proceeding with next exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfe861ea-9235-4111-ae94-60b40974d4a2",
      "metadata": {
        "id": "cfe861ea-9235-4111-ae94-60b40974d4a2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os._exit(0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4987cfbe06fd42e2951d7075467df45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcdf19b88ab74a159615fd5eafc202c3",
              "IPY_MODEL_29c5933c48a849b2aa03c61ef9ebc527"
            ],
            "layout": "IPY_MODEL_8651add1fcf94edd997c15f9389b2943"
          }
        },
        "bcdf19b88ab74a159615fd5eafc202c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "HF Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3c96dc3c637c40ab82c70a5ee8d48214",
            "placeholder": "paste your Hugging Face token here",
            "style": "IPY_MODEL_1548cef0d62f4aacaa0d7cd00af39934",
            "value": ""
          }
        },
        "29c5933c48a849b2aa03c61ef9ebc527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Save",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_a462a77199e94aed983aa08ca90fa08a",
            "style": "IPY_MODEL_a9a1d33041294155abfbd223788e680d",
            "tooltip": ""
          }
        },
        "8651add1fcf94edd997c15f9389b2943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c96dc3c637c40ab82c70a5ee8d48214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "450px"
          }
        },
        "1548cef0d62f4aacaa0d7cd00af39934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a462a77199e94aed983aa08ca90fa08a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9a1d33041294155abfbd223788e680d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c5285a0ac0c046e189699334ea6dbee8": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_9a80a76bef1c421984241acd563bc05c",
            "msg_id": "",
            "outputs": []
          }
        },
        "9a80a76bef1c421984241acd563bc05c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4bccb87a7c44924937b4345590a6ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f3c5b6f3e2f46a7ac2656843cf831a8",
              "IPY_MODEL_5d24555192ed4cf8b878153ac30af650",
              "IPY_MODEL_07a71c9eee734d51b6ad7e720bfe6da0"
            ],
            "layout": "IPY_MODEL_3990fd1020ce4ce3ade8050981ba1f0c"
          }
        },
        "1f3c5b6f3e2f46a7ac2656843cf831a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe5a17e537dc4bc6930f740519e53b7d",
            "placeholder": "​",
            "style": "IPY_MODEL_749bd950745b4df38b0e5dec19f8e5d7",
            "value": "tokenizer_config.json: "
          }
        },
        "5d24555192ed4cf8b878153ac30af650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0786f00d19ae41978e7ecaf622a2c8f9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c94dafd4dd2c4b80a16e52ebe7330b61",
            "value": 1
          }
        },
        "07a71c9eee734d51b6ad7e720bfe6da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad5e9a37d61d42f597fb807921860f47",
            "placeholder": "​",
            "style": "IPY_MODEL_3157bfa3d948448ba12eebf02778fe10",
            "value": " 181k/? [00:00&lt;00:00, 15.5MB/s]"
          }
        },
        "3990fd1020ce4ce3ade8050981ba1f0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe5a17e537dc4bc6930f740519e53b7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "749bd950745b4df38b0e5dec19f8e5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0786f00d19ae41978e7ecaf622a2c8f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c94dafd4dd2c4b80a16e52ebe7330b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad5e9a37d61d42f597fb807921860f47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3157bfa3d948448ba12eebf02778fe10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de5163986aa0454ba75eb9cc5e8dea98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12b6d13a119048d09662a8e23eaecd5f",
              "IPY_MODEL_414d6e2303254546b657f9cad1322d87",
              "IPY_MODEL_a703b9deeebb429c83d9231b37bf271e"
            ],
            "layout": "IPY_MODEL_4aa94dc1333645b5b4095f97c39e8f20"
          }
        },
        "12b6d13a119048d09662a8e23eaecd5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27d4db0e87e14803a12195289653fead",
            "placeholder": "​",
            "style": "IPY_MODEL_43210d55e50546d594795ccf47d2ddc8",
            "value": "tokenizer.json: 100%"
          }
        },
        "414d6e2303254546b657f9cad1322d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccb8168e0ef549e0b0adf64e463e341e",
            "max": 17078330,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb6f5076b56640ae97f819f9c5bac626",
            "value": 17078330
          }
        },
        "a703b9deeebb429c83d9231b37bf271e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9226a09cef9d4a3a842c15836dc32a38",
            "placeholder": "​",
            "style": "IPY_MODEL_3bf3a729f60a4900ac198dbd3e73dcc4",
            "value": " 17.1M/17.1M [00:00&lt;00:00, 25.9MB/s]"
          }
        },
        "4aa94dc1333645b5b4095f97c39e8f20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27d4db0e87e14803a12195289653fead": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43210d55e50546d594795ccf47d2ddc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccb8168e0ef549e0b0adf64e463e341e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb6f5076b56640ae97f819f9c5bac626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9226a09cef9d4a3a842c15836dc32a38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bf3a729f60a4900ac198dbd3e73dcc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33c05b832ef7403fbe234bca1ab8c788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc3c9469f7ff4501ad7eddbfa6adac5a",
              "IPY_MODEL_bd3d2f0f3c9a4d55b0ef7be11a37ad0d",
              "IPY_MODEL_ad1bd798abb1468e811583a4170f4dad"
            ],
            "layout": "IPY_MODEL_cb140bcb9b084230b218d615952d1c0d"
          }
        },
        "bc3c9469f7ff4501ad7eddbfa6adac5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e70e4f7e0c4496ead3e5877d04d895f",
            "placeholder": "​",
            "style": "IPY_MODEL_a906165a85b3424bbafd4e671d752c4a",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "bd3d2f0f3c9a4d55b0ef7be11a37ad0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61bd8c7bcfae4cc7894dad9c9aac20a7",
            "max": 422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db09255757ec4808b6699aa9521237e1",
            "value": 422
          }
        },
        "ad1bd798abb1468e811583a4170f4dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a903bec7f940fe92f5cad4875072bf",
            "placeholder": "​",
            "style": "IPY_MODEL_4db67f5605384218a952c3e38375f4a0",
            "value": " 422/422 [00:00&lt;00:00, 56.3kB/s]"
          }
        },
        "cb140bcb9b084230b218d615952d1c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e70e4f7e0c4496ead3e5877d04d895f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a906165a85b3424bbafd4e671d752c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61bd8c7bcfae4cc7894dad9c9aac20a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db09255757ec4808b6699aa9521237e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1a903bec7f940fe92f5cad4875072bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4db67f5605384218a952c3e38375f4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daacc8fdb61b4927bf6636e4870c7230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49c27477e1f7474ab77ec87b8c824e53",
              "IPY_MODEL_8cb5a2b092294a9d924f3a3e9ef071d4",
              "IPY_MODEL_e3b7f43ad071486a8c46f04ac0544834"
            ],
            "layout": "IPY_MODEL_360a1687b25d472f8c81d02a4ee3667e"
          }
        },
        "49c27477e1f7474ab77ec87b8c824e53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbacb77bd01c474abb46f3d95b26b072",
            "placeholder": "​",
            "style": "IPY_MODEL_9d3885d29c4d4e61b4f59396fe688091",
            "value": "config.json: "
          }
        },
        "8cb5a2b092294a9d924f3a3e9ef071d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb7693ff714b48ba850ca7da216ea467",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0edf80f715194414b1d506373adfeae9",
            "value": 1
          }
        },
        "e3b7f43ad071486a8c46f04ac0544834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7917f6c56d5847c2bb1fe94d87f40cbf",
            "placeholder": "​",
            "style": "IPY_MODEL_6fcc4ec9bad44c8a9274fd1fbce2e5ea",
            "value": " 1.56k/? [00:00&lt;00:00, 152kB/s]"
          }
        },
        "360a1687b25d472f8c81d02a4ee3667e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbacb77bd01c474abb46f3d95b26b072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d3885d29c4d4e61b4f59396fe688091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb7693ff714b48ba850ca7da216ea467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0edf80f715194414b1d506373adfeae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7917f6c56d5847c2bb1fe94d87f40cbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fcc4ec9bad44c8a9274fd1fbce2e5ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce887141403e48e0b0aa71172e0f41fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e68b518536b5464b990eb64ad86463b4",
              "IPY_MODEL_ffd958bf1f7b4a49b86052d3da296b32",
              "IPY_MODEL_36b876a0ec8e449ea6821ca6582ec35e"
            ],
            "layout": "IPY_MODEL_0a5316227ea345e9a71e47ad8a11899d"
          }
        },
        "e68b518536b5464b990eb64ad86463b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_384db9377d634ee394a3d177b7fab273",
            "placeholder": "​",
            "style": "IPY_MODEL_61691f376c8f4b70820436ec08be6af5",
            "value": "configuration_nemotron_h.py: "
          }
        },
        "ffd958bf1f7b4a49b86052d3da296b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0093909427004f278c6588cd9f4ffa23",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a963db7ebae4f4ab7577cb580b1a627",
            "value": 1
          }
        },
        "36b876a0ec8e449ea6821ca6582ec35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d463ad16e2de4388a81bd588d9f07172",
            "placeholder": "​",
            "style": "IPY_MODEL_20a263f6919e407682f4a8d98570aa31",
            "value": " 12.2k/? [00:00&lt;00:00, 1.02MB/s]"
          }
        },
        "0a5316227ea345e9a71e47ad8a11899d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "384db9377d634ee394a3d177b7fab273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61691f376c8f4b70820436ec08be6af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0093909427004f278c6588cd9f4ffa23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5a963db7ebae4f4ab7577cb580b1a627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d463ad16e2de4388a81bd588d9f07172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20a263f6919e407682f4a8d98570aa31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47fb0ba1b1e14899b470eedac2d2fc48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c8d0ab350a44fecbdd15923ec45f127",
              "IPY_MODEL_ced30d1a2da34f43ac4c37817f7846bd",
              "IPY_MODEL_e1c81d824ad8433991bb4a5a3301c41b"
            ],
            "layout": "IPY_MODEL_b8f136e882f7462b9bbbd3559409108a"
          }
        },
        "4c8d0ab350a44fecbdd15923ec45f127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_004981eb2df84f16bfc0e06c6b409abb",
            "placeholder": "​",
            "style": "IPY_MODEL_87d4dc8285564d1680b41c883f1b86eb",
            "value": "generation_config.json: 100%"
          }
        },
        "ced30d1a2da34f43ac4c37817f7846bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_410733531c4d4102ac01bd535e815471",
            "max": 158,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_537ca91504194fc0b2a88ede82bbec4a",
            "value": 158
          }
        },
        "e1c81d824ad8433991bb4a5a3301c41b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6de58b23471b49baa3fc9eaa7e2a5465",
            "placeholder": "​",
            "style": "IPY_MODEL_59d6e3f02e8442208ff41dcefbf9dc4d",
            "value": " 158/158 [00:00&lt;00:00, 5.52kB/s]"
          }
        },
        "b8f136e882f7462b9bbbd3559409108a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "004981eb2df84f16bfc0e06c6b409abb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87d4dc8285564d1680b41c883f1b86eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "410733531c4d4102ac01bd535e815471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "537ca91504194fc0b2a88ede82bbec4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6de58b23471b49baa3fc9eaa7e2a5465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59d6e3f02e8442208ff41dcefbf9dc4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}